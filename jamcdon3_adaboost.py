# -*- coding: utf-8 -*-
"""jamcdon3_adaboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ygd6CvFS4C0jqaks2Z0ZB8XP3cBfAad3
"""


import numpy as np
from math import log
import matplotlib.pyplot as plt
import cv2
import os
import time

# Adaboost Algorithm

# Size x Size of data
window_size = 20

# Type      7 types based on haar shape
# Feature   cv2 of feature type
# Start     starting pixel of feature on image
# Size      ratio for increased size of feature
# Shape     original shape of feature
class Haar(object):
    def __init__(self, type, feature, size, shape, start):
        self.type = type
        self.feature = cv2.resize(feature, (shape[1]*size, shape[0]*size), interpolation=cv2.INTER_NEAREST)
        self.start = start
        self.size = size
        self.shape = shape


# Haar      associated haar object
# Theta     optimal threshold for classifier
# Sign      polarity of classifier, used in prediction
# Weight    Not used
class WeakClassifier(object):
    def __init__(self, haar, theta, sign, weight):
        self.haar = haar
        self.theta = theta
        self.sign = sign
        self.weight = weight


def blockSum(integral, w, h, w_kernel_size, h_kernel_size):
    w = int(w)
    h = int(h)
    w_kernel_size = int(w_kernel_size)
    h_kernel_size = int(h_kernel_size)
    endw = w + w_kernel_size - 1
    endh = h + h_kernel_size - 1
    sum = integral[endh, endw]
    if w > 0:
        sum -= integral[endh, w - 1]
    if h > 0:
        sum -= integral[h - 1, endw]
    if h > 0 and w > 0:
        sum += integral[h - 1, w - 1]
    return sum


def subwindow_integral(subwindow):
    integral = np.zeros(subwindow.shape)

    for r in range(subwindow.shape[0]):
        for c in range(subwindow.shape[1]):
            greyIntegralVal = subwindow[r, c]

            if r - 1 >= 0 and c - 1 >= 0:
                greyIntegralVal -= integral[r-1, c-1]

            if r - 1 >= 0:
                greyIntegralVal += integral[r-1, c]

            if c - 1 >= 0:
                greyIntegralVal += integral[r, c-1]

            integral[r, c] = greyIntegralVal

    return integral


def get_haar_score_fast(haar, subwindow, subwindow_integral):
    r = haar.start[0]
    c = haar.start[1]
    size_w = haar.feature.shape[1]
    size_h = haar.feature.shape[0]

    if size_w == 2 and size_h == 2:
        # Simple feature, not using integral
        if haar.type == 1:
            return subwindow[r, c]+subwindow[r+1, c]-subwindow[r, c+1]-subwindow[r+1, c+1]
        elif haar.type == 2:
            return subwindow[r, c]+subwindow[r, c+1]-subwindow[r+1, c]-subwindow[r+1, c+1]
        elif haar.type == 5:
            return subwindow[r, c]+subwindow[r+1, c+1]-subwindow[r, c+1]-subwindow[r+1, c]
        elif haar.type == 6:
            return subwindow[r+1, c]+subwindow[r, c+1]-subwindow[r, c]-subwindow[r+1, c+1]
    else:
        # More complex features, using integral to reduce array references
        if haar.type == 1:
            return blockSum(subwindow_integral, c, r, size_w / 2, size_h) - blockSum(subwindow_integral, c + (size_w / 2), r, size_w / 2, size_h)
        elif haar.type == 2:
            return blockSum(subwindow_integral, c, r, size_w, size_h / 2) - blockSum(subwindow_integral, c, r + (size_h / 2), size_w, size_h / 2)
        elif haar.type == 3:
            return blockSum(subwindow_integral, c, r, size_w, size_h) - 2 * blockSum(subwindow_integral, c + (size_w / 3), r, size_w / 3, size_h)
        elif haar.type == 4:
            return blockSum(subwindow_integral, c, r, size_w, size_h) - 2 * blockSum(subwindow_integral, c, r + (size_h / 3), size_w, size_h / 3)
        elif haar.type == 5:
            return blockSum(subwindow_integral, c, r, size_w, size_h) - 2 * (blockSum(subwindow_integral, c + (size_w / 2), r, size_w / 2, size_h / 2) + blockSum(subwindow_integral, c, r + (size_h / 2), size_w / 2, size_h / 2))
        elif haar.type == 6:
            return blockSum(subwindow_integral, c, r, size_w, size_h) - 2 * (blockSum(subwindow_integral, c, r, size_w / 2, size_h / 2) + blockSum(subwindow_integral, c + (size_w / 2), r + (size_h / 2), size_w / 2, size_h / 2))
        elif haar.type == 7:
            if size_w == 3:
                return blockSum(subwindow_integral, c, r, size_w, size_h) - 2 * subwindow[r + 1, c + 1]
            else:
                return blockSum(subwindow_integral, c, r, size_w, size_h) - 2 * blockSum(subwindow_integral, c + (size_w / 3), r + (size_h / 3), size_w / 3, size_h / 3)


def feature_weighted_error_rate(actual, predicted, weights):
    return sum(weights*(np.not_equal(actual, predicted)))


def predict(score, classifier):
    if score < classifier.theta:
        return -classifier.sign
    return classifier.sign

def get_samples(path):
    samples = []
    for file in os.listdir(path):
        samples.append(cv2.imread(path + file, cv2.IMREAD_GRAYSCALE))
    return samples


def plot_haar(haar, title):
    plt.style.use('grayscale')
    plt.axis('off')
    plt.title(title)
    plt.imshow(haar, interpolation='nearest')
    plt.show()

# Generate positive samples
# Generate positive samples
positive_samples = get_samples("positive_samples\\")

# Generate Negative samples
negative_samples = get_samples("negative_samples\\")

negative_samples_rotated = [np.rot90(np.rot90(np.rot90(el))) for el in negative_samples]
negative_samples = negative_samples + negative_samples_rotated

# Shuffle all the samples
np.random.shuffle(positive_samples)
np.random.shuffle(negative_samples)

print("Total samples: ")
print("Positives: " + str(len(positive_samples)))
print("Negatives: " + str(len(negative_samples)))

split = 0.90
pos_split = int(len(positive_samples)*split)
neg_split = int(len(negative_samples)*split)

training_set = positive_samples[0:pos_split] + negative_samples[0:neg_split]
testing_set = positive_samples[pos_split:] + negative_samples[neg_split:]

training_integrals = [subwindow_integral(el) for el in training_set]
testing_integrals = [subwindow_integral(el) for el in testing_set]

training_set_pos = pos_split
training_set_neg = neg_split

testing_set_pos = len(positive_samples) - training_set_pos
testing_set_neg = len(negative_samples) - training_set_neg

training_labels = [1] * training_set_pos + [-1] * training_set_neg
testing_labels = [1] * testing_set_pos + [-1] * testing_set_neg

print("For training")
print("Positives: " + str(training_set_pos))
print("Negatives: " + str(training_set_neg))

print("For testing")
print("Positives: " + str(testing_set_pos))
print("Negatives: " + str(testing_set_neg))

print("Testing training split: " + str(split))

# Generate many haar features
features_start = []

# Define haar feature types
haar1 = np.array([1, -1,
                  1, -1])
haar1.shape = (2, 2)

haar2 = np.array([-1, -1,
                  1, 1])
haar2.shape = (2, 2)

haar3 = np.array([-1, 1, -1,
                  -1, 1, -1])
haar3.shape = (2, 3)

haar4 = np.array([1, 1,
                  -1, -1,
                  1, 1])
haar4.shape = (3, 2)

haar5 = np.array([1, -1,
                  -1, 1])
haar5.shape = (2, 2)

haar6 = np.array([-1, 1,
                  1, -1])
haar6.shape = (2, 2)

haar7 = np.array([1, 1, 1,
                  1, -1, 1,
                  1, 1, 1])
haar7.shape = (3, 3)

haar_feature_types = [haar1, haar2, haar3, haar4, haar5, haar6, haar7]

# Plot in matlib the haar features
for f in range(len(haar_feature_types)):
    plot_haar(haar_feature_types[f], 'Haar ' + str(f+1))

# Define many sizes for all feature types
for f in range(len(haar_feature_types)):
    shape = haar_feature_types[f].shape
    if 3 in haar_feature_types[f].shape:
        max_size = 4
    else:
        max_size = 7

    # Generate features for different haar sizes
    for s in range(1, max_size+1):
        features_start.append(Haar(f+1, haar_feature_types[f], s, shape, (0,0)))

features = []

# Populate feature vector with all possible starting locations and sizes
for j in features_start:
    starting_positions = []
    space = (window_size-j.shape[0]*j.size, window_size-j.shape[1]*j.size)
    for m in range(space[0]+1):
        for n in range(space[1]+1):
            starting_positions.append((m, n))

    for location in starting_positions:
        features.append(Haar(j.type, j.feature, j.size, j.shape, location))

features = list(features)

feature_weights = []

np.random.shuffle(features)

scores = []
polarities = []
errors = []
thetas = []

# For each feature, get scores and polarity and optimal threshold
for j in features:
    print('Feature ' + str(features.index(j)+1) + ' out of: ' + str(len(features)))
    avgPosScore = 0.0
    avgNegScore = 0.0

    # Apply feature to every training sample and get average score for feature
    for i in range(len(training_set)):
        score = get_haar_score_fast(j, training_set[i], training_integrals[i])
        scores.append(score)

        if training_labels[i] == 1:
            avgPosScore += score
        else:
            avgNegScore += score

    avgPosScore = avgPosScore / training_set_pos
    avgNegScore = avgNegScore / training_set_neg
    if avgPosScore > avgNegScore:
        polarity = 1
    else:
        polarity = -1
    polarities.append(polarity)

    # Optimal theta found
    theta = (avgPosScore + avgNegScore) / 2
    thetas.append(theta)

# AdaBoost Algorithm and Cascade Creation

F_target = 0.05
f = 0.5

F_i = 1

cascade = []
start_time = time.time()

image_weights = [1.0 / (2 * training_set_pos)] * training_set_pos + [1.0 / (2 * training_set_neg)] * training_set_neg

# Iterate until F_target is reached
while F_i > F_target:

    index_next_feature = 0
    next_weak_classifier = 0
    lowest_error = float("inf")

    # initialize image weights
    total = sum(image_weights)
    image_weights = [w / total for w in image_weights]

    TP = 0
    TN = 0

    f_i = 1
    loop = 0

    while (TP / training_set_pos < 0.5) and (TN / training_set_neg < 0.5):
        total = sum(image_weights)
        if total != 1:
            image_weights = [w / total for w in image_weights]

        errors = []
        counter = 0
        inner_counter = 0

        # Get Weighted Error for each Feature. Find next best feature.
        for j in features:

            # Create object for each feature
            w_classif = WeakClassifier(j, thetas[counter], polarities[counter], 0)

            # Get predictions of all samples
            predicted = []
            for sample in range(len(training_set)):
                score = scores[inner_counter]
                predicted.append(predict(score, w_classif))
                inner_counter += 1

            # Calculate weighted error for each feature
            weighted_error = feature_weighted_error_rate(training_labels, predicted, image_weights)
            errors.append(weighted_error)

            # Select feature with lowest error as next classifier in cascade
            if weighted_error < lowest_error:
                lowest_error = weighted_error
                next_weak_classifier = w_classif
                index_next_feature = features.index(j)

            counter += 1

        print(" ")
        print("Best feature index: " + str(index_next_feature) + "/" + str(len(features)))

        plt.plot(errors)
        plt.ylabel("Weighted Feature Error")
        plt.xlabel("Total Features")
        plt.title("Pick the next 'best' weak classifier")
        plt.show()

        # Choose weak classifier with lowest error
        b_t = lowest_error / (1 - lowest_error)

        if b_t == 0:
            invert_weight = 0
        else:
            invert_weight = log(1 / b_t)
        next_weak_classifier.weight = invert_weight

        # Update weights and evaluate current weak classifier
        predicted = []
        scores_weighted = []
        for sample in range(len(training_set)):
            # Get weighted classification error
            score = get_haar_score_fast(next_weak_classifier.haar, training_set[sample], training_integrals[sample])
            scores_weighted.append(score)
            predicted.append(predict(score, next_weak_classifier))

        FP = 0.0
        FN = 0.0
        TP = 0.0
        TN = 0.0
        colors_predicted = []

        for k in range(len(image_weights)):
            # if sample is correctly classified

            if training_labels[k] == 1 and predicted[k] == -1:
                FN += 1
            if training_labels[k] == -1 and predicted[k] == 1:
                FP += 1

            # Update image weights
            if training_labels[k] == predicted[k]:
                image_weights[k] = image_weights[k] * b_t
                if predicted[k] == 1:
                    TP += 1
                if predicted[k] == -1:
                    TN += 1

            if predicted[k] == -1:
                colors_predicted.append('r')
            else:
                colors_predicted.append('b')

        # Evaluate f_i
        f_i = (FP / (2 * training_set_neg)) + (FN / (2 * training_set_pos))
        print("f_i: " + str(f_i))

        print("TP, TN, FP, FN for the current weak classifier:")
        print(TP / training_set_pos, TN / training_set_neg, FP / training_set_neg, FN / training_set_pos)

        print('Threshold: ' + str(next_weak_classifier.theta))
        print('Classification error: ' + str((FP + FN) / (TP + TN + FP + FN)))

        # Visualize the performance of weak classifier for training samples
        plt.scatter(range(training_set_pos + training_set_neg), scores_weighted, c=colors_predicted)
        plt.vlines(training_set_pos, min(scores_weighted), max(scores_weighted))
        plt.plot(range(training_set_pos + training_set_neg),
                 [next_weak_classifier.theta] * (training_set_pos + training_set_neg))
        plt.xlim(0, training_set_pos + training_set_neg)
        plt.ylabel("Weighted Classification Error")
        plt.xlabel("Training Samples")
        plt.title("Classification Performance for Weak Classifier " + str(len(cascade) + 1))
        plt.show()

        loop += 1

    cascade.append(next_weak_classifier)

    # Plot in matlib the haar features
    plot_haar(next_weak_classifier.haar.feature, "Weak Classifier Size " +
              str(next_weak_classifier.haar.shape[1] * next_weak_classifier.haar.size) +
              " x " + str(next_weak_classifier.haar.shape[0] * next_weak_classifier.haar.size))

    strong_FP = 0.0
    strong_FN = 0.0

    cascade_scores = []
    cascade_colors_predicted = []
    for l in range(len(training_set)):
        strong_score = 0.0
        for w_class in cascade:
            strong_score += w_class.weight * predict(
                get_haar_score_fast(w_class.haar, training_set[l], training_integrals[l]), w_class)
        cascade_scores.append(strong_score)
        classify = np.sign(strong_score)
        if classify == -1:
            cascade_colors_predicted.append('r')
        else:
            cascade_colors_predicted.append('b')

        if training_labels[l] == 1 and classify == -1:
            strong_FN += 1
        if training_labels[l] == -1 and classify == 1:
            strong_FP += 1

    # Visualize the performance of the cascade on training samples
    plt.scatter(range(training_set_pos + training_set_neg), cascade_scores, c=cascade_colors_predicted)
    plt.vlines(training_set_pos, min(cascade_scores), max(cascade_scores))
    plt.plot(range(training_set_pos + training_set_neg), [0] * (training_set_pos + training_set_neg))
    plt.xlim(0, training_set_pos + training_set_neg)
    plt.ylabel("Cascade Score")
    plt.xlabel("Training Samples")
    plt.title("Classification Performance for Cascade of Size " + str(len(cascade)))
    plt.show()

    F_i = (strong_FP / (2 * training_set_neg)) + (strong_FN / (2 * training_set_pos))
    print("F_i: " + str(F_i))
    print("Cascade size: " + str(len(cascade)))

print("--- %s seconds ---" % (time.time() - start_time))

# Run Strong Classifier on Testing Set

FP_test = 0.0
FN_test = 0.0
TP_test = 0.0
TN_test = 0.0

scores = []

for t in range(len(testing_set)):
    strong_score = 0.0
    for w_class in cascade:
        strong_score += w_class.weight * predict(get_haar_score_fast(w_class.haar, testing_set[t], testing_integrals[t]), w_class)
    classify = np.sign(strong_score)
    scores.append(strong_score)

    if testing_labels[t] == 1 and classify == -1:
        FN_test += 1
    if testing_labels[t] == -1 and classify == 1:
        FP_test += 1
    if testing_labels[t] == 1 and classify == 1:
        TP_test += 1
    if testing_labels[t] == -1 and classify == -1:
        TN_test += 1

# Display Evaluation of Strong Classifier

print("Result of Strong Classifier on Testing Set")
print("Test F_i: " + str((FP_test / (2 * testing_set_neg)) + (FN_test / (2 * testing_set_pos))))
print("False Positives for Testing: " + str(FP_test))
print("False Negatives for Testing: " + str(FN_test))

plt.plot(range(testing_set_pos), scores[0:testing_set_pos], 'bo')
plt.vlines(testing_set_pos, min(scores), max(scores))
plt.plot(range(testing_set_pos, testing_set_pos + testing_set_neg), scores[testing_set_pos:], 'ro')
plt.plot(range(testing_set_pos + testing_set_neg), [0] * (testing_set_pos + testing_set_neg))
plt.ylabel("Strong Classifier Score")
plt.xlabel("Testing Samples")
plt.title("Performance of Cascade on Testing Set")
plt.show()

print("TP Rate: " + str(TP_test / testing_set_pos))
print("TN Rate: " + str(TN_test / testing_set_neg))
print("FP Rate: " + str(FP_test / testing_set_neg))
print("FN Rate: " + str(FN_test / testing_set_pos))

# Plotting ROC Graph

FPR = []
TPR = []

print('Number of strong scores / testing set size: ' + str(len(scores)))
print('Maximum strong score: ' + str(max(scores)))
print('Minimum strong score: ' + str(min(scores)))
for n in np.arange(-round(max(scores)),-round(min(scores)),1):

  FP_test = 0.0
  FN_test = 0.0
  TP_test = 0.0
  TN_test = 0.0

  for t in range(len(scores)):
    classify = np.sign(scores[t]+n)
    if testing_labels[t] == 1 and classify == -1:
        FN_test += 1
    if testing_labels[t] == -1 and classify == 1:
        FP_test += 1
    if testing_labels[t] == 1 and classify == 1:
        TP_test += 1
    if testing_labels[t] == -1 and classify == -1:
        TN_test += 1

  TPR.append(TP_test/(TP_test + FN_test))
  FPR.append(FP_test/(FP_test + TN_test)) 

print(TPR)
print(FPR)

plt.plot([0,1], 'r--', FPR, TPR, 'b--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC of thresholding F(x)')
plt.show()